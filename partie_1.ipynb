{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atelier PyTorch : les bases pour être autonome \n",
    "#### 3 novembre 2022 de 9h à 17h à l'OMP (salle Coriolis)\n",
    "\n",
    "L'objectif de cet atelier est de comprendre le fonctionnement de la bibliothèque PyTorch et de s'en approprier les bases pour être autonome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie 1\n",
    "## Manipuler les objets basiques de PyTorch : tenseurs, paramètres, modèles... \n",
    "\n",
    "Tout au long de l'atelier, nous travaillerons avec un modèle classique dans la litérature du machine learning : AlexNet [1], un réseau de neurones profond à convolutions pour la classification d'images naturelles (voir exemples d'images des données ImageNet ci-dessous).\n",
    "\n",
    "Dans cette première partie, nous allons : \n",
    " * Définir un **modèle PyTorch** de AlexNet,\n",
    " * Explorer son architecture et en particulier, faire la différence entre les **tenseurs** et les **paramètres** qui le constituent,\n",
    " * Apprendre les opérations basiques sur les tenseurs,\n",
    " * Charger dans le modèle des paramètres **pré-entrainés**,\n",
    " * **Visualiser** les noyaux de convolution,\n",
    " * Définir notre **propre** couche de convolution.\n",
    " \n",
    "[1] Krizhevsky, Alex et al. “ImageNet classification with deep convolutional neural networks.” Communications of the ACM 60 (2012): 84 - 90.\n",
    " \n",
    "<img src=\"data/image_net.png\" width=500 height=200 />\n",
    "\n",
    "<center>Exemples d'images de ImageNet</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définir un modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    \"\"\"\n",
    "    AlexNet model architecture from the <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "    Credits to https://github.com/Lornatang/AlexNet-PyTorch \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, dropout_rate=0):\n",
    "        # Notre modèle, comme tous les réseaux de neurones dans PyTorch,\n",
    "        # hérite de la classe générique nn.Module.\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        # A l'initialisation, on définit les couches du modèle, les fonctions\n",
    "        # d'activation, etc. \n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "        # On définit aussi les distributions de probabilité selon lesquelles\n",
    "        # les paramètres du modèle vont être initialisés.\n",
    "\n",
    "        for m in self.modules():\n",
    "            # Les paramètres sont initialisés différemment \n",
    "            # selon le type de couche.\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "    def extract_features(self, inputs):\n",
    "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
    "        x = self.features(inputs)\n",
    "        return x\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.features(inputs)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.named_parameters() renvoie un générateur de tuples contenant le nom et la valeur des paramètres d'une instance de nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x[0], x[1].shape) for x in model.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.parameters() renvoie un générateur contenant uniquement la valeur des paramètres d'une instance de nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[param.shape for param in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_params(model):\n",
    "    n = 0\n",
    "    for param in model.parameters():\n",
    "        n += param.shape.numel()\n",
    "    return n\n",
    "\n",
    "print(\"{:.2e}\".format(n_params(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.features[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features[0].weight.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le  tenseur, l'élément de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = model.features[0].weight.data\n",
    "type(kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tenseur (torch.Tensor) est l'équivalent du array de numpy. On peut souvent y appliquer les mêmes méthodes telle que .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_kernels = kernels.view(-1, 11, 11)\n",
    "reshaped_kernels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".view est l'équivalent de .reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_kernels_over_rgb = torch.mean(kernels, dim=1)\n",
    "avg_kernels_over_rgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également facilement ajouter ou retirer une dimension comme suit :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kernels.shape)\n",
    "kernels = kernels.unsqueeze(0)\n",
    "print(kernels.shape)\n",
    "kernels = kernels.squeeze(0)\n",
    "print(kernels.shape)\n",
    "kernels = kernels.unsqueeze(1)\n",
    "print(kernels.shape)\n",
    "kernels = kernels.squeeze(1)\n",
    "print(kernels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également moyenner ou sommer de la même manière qu'avec numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fonctions d'activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.features[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.features[4].inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger des paramètres pré-entrainés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = model.to(device) # On charge le modèle sur GPU si elle est disponible\n",
    "pretrained_weights = torch.load('alex_net/pretrained_model.pth.tar', \n",
    "                                map_location=device) # on précise le \"device\" sur lequel est le modèle\n",
    "model.load_state_dict(pretrained_weights['state_dict'])\n",
    "del pretrained_weights # On supprime les poids de la mémoire du GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(8,8, figsize=(10,10))\n",
    "k = 0\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        ax[i,j].imshow(model.features[0].weight.data[k,0,:,:].cpu(), cmap='gray')\n",
    "        # model.features[0].weight.data is of size (64, 3, 11, 11)\n",
    "        ax[i,j].set_xticks([])\n",
    "        ax[i,j].set_yticks([])\n",
    "        k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définir sa propre couche de convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les noyaux de convolution ci-dessus ressemblent à des filtres de Gabor. On pourrait initialiser les noyaux de convolution comme des filtres de Gabor ou même optimiser des filtres de Gabor comme proposé dans [2] plutôt que des noyaux classiques.\n",
    "\n",
    "Dans cette sous-partie, on va définir notre propre couche de filtre de Gabor. Précisémment, on va implémenter la partie réelle d'un filtre de Gabor qui est définie comme suit : \n",
    "\n",
    "$$g(x,y; \\lambda, \\psi, \\sigma, \\gamma, \\theta) = exp(-\\frac{x'^2 + \\gamma^2 \\cdot y'^2}{2\\sigma^2}) \\cdot cos(2\\pi \\frac{x'}{\\lambda} + \\psi)$$\n",
    "\n",
    "$\\lambda$, $\\psi$, $\\sigma$ et $\\gamma$ seront des paramètres à optimiser. $\\theta$ prendra plusieurs valeurs déterminées pour obtenir des filtres avec différentes orientations.\n",
    "\n",
    "[2] Alekseev, Andrey, and Anatoly Bobe. \"GaborNet: Gabor filters with learnable parameters in deep convolutional neural network.\" 2019 International Conference on Engineering and Telecommunication (EnT). IEEE, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En fait, on souhaiterait conserver les propriétés et fonctions d'une couche de convolution classique, mais en redéfinissant uniquement les poids de la convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va donc écire une classe qui hérite de torch.nn.modules.conv.Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compléter le code à chaque \"...\" \n",
    "class GaborFilters(torch.nn.modules.conv.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=False,\n",
    "                 padding_mode='zeros', device=None, dtype=None, n_thetas=5):\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.bias_ = bias\n",
    "        self.padding_mode = padding_mode\n",
    "        self.device = device\n",
    "        self.dtype=dtype\n",
    "        self.n_thetas = self.out_channels//16\n",
    "        \n",
    "        super().__init__(...)\n",
    "        \n",
    "        # A compléter \n",
    "        delattr(self, ...) \n",
    "        \n",
    "        lambda_init_values = 1e-1+torch.rand(self.out_channels//self.n_thetas, 1, self.in_channels, 1, 1)*32\n",
    "        psi_init_values = torch.rand(self.out_channels//self.n_thetas, 1, self.in_channels, 1, 1)*math.pi/2\n",
    "        sigma_init_values = 1e-1+torch.rand(self.out_channels//self.n_thetas, 1, self.in_channels, 1, 1)*10\n",
    "        gamma_init_values = torch.rand(self.out_channels//self.n_thetas, 1, self.in_channels, 1, 1)\n",
    "        \n",
    "        # A compléter\n",
    "        self.lambda_ = ...\n",
    "        self.psi = ...\n",
    "        self.sigma = ...\n",
    "        self.gamma = ...\n",
    "        \n",
    "        thetas = torch.linspace(0., 2*math.pi, self.n_thetas)\n",
    "        \n",
    "        # A compléter \n",
    "        self.register_buffer(...)\n",
    "        \n",
    "    @property\n",
    "    def weight(self):\n",
    "        # A compléter\n",
    "\n",
    "    def gabor_filter(self):\n",
    "        x = torch.arange(self.kernel_size[0], dtype=torch.float32) -  (self.kernel_size[0] - 1)/2\n",
    "        # A compléter \n",
    "        x = x.view(...)\n",
    "        y = torch.arange(self.kernel_size[0], dtype=torch.float32) -  (self.kernel_size[0] - 1)/2\n",
    "        # A compléter\n",
    "        y = y.view(...)\n",
    "        \n",
    "        # A compléter\n",
    "        thetas = self.thetas.view(...)\n",
    "\n",
    "        x_ =  x * torch.cos(thetas) + y * torch.sin(thetas)\n",
    "        y_ = -x * torch.sin(thetas) + y * torch.cos(thetas)\n",
    "\n",
    "        gb = torch.exp(-0.5 * ((x_ ** 2 + self.gamma**2 * y_ ** 2) / self.sigma ** 2)) \\\n",
    "                 * torch.cos(2.0 * math.pi  * x_ / self.lambda_ + self.psi)\n",
    "        \n",
    "        gb = gb.view(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])\n",
    "\n",
    "        return gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GaborFilters(3, 64, 11)\n",
    "kernels = gb.weight.cpu().detach()\n",
    "\n",
    "fig, ax = plt.subplots(8,8, figsize=(10,10))\n",
    "k = 0\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        ax[i,j].imshow(kernels[k,0,:,:], cmap='gray')\n",
    "        ax[i,j].set_xticks([])\n",
    "        ax[i,j].set_yticks([])\n",
    "        k += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 50, 50)\n",
    "out = gb(x)\n",
    "loss = torch.linalg.norm(out)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Weight: \", gb.weight.mean())\n",
    "\n",
    "optimizer = torch.optim.Adam(gb.parameters(), lr=0.1)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(\"Weight: \", gb.weight.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
